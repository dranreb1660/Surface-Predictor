{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b16bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "from pathlib import Path\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "%cd \"/content/drive/Othercomputers/my_mac/Coding_stuff/Projects/Surface_Predictor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9032b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = '{\"username\":\"phade160\",\"key\":\"e780a54d20d6c7d34b674893c923068a\"}'\n",
    "cred_path = Path('~/.kaggle/kaggle.json').expanduser()\n",
    "if not cred_path.exists():\n",
    "  print('entered')\n",
    "cred_path.parent.mkdir(exist_ok=True)\n",
    "cred_path.write_text(creds)\n",
    "cred_path.chmod(0o600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723dbd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c career-con-2019 -p ./data/\n",
    "# !unzip ./data/X_test.csv.zip -d ./data\n",
    "# !unzip ./data/X_train.csv.zip -d ./data\n",
    "# !rm ./data/X_train.csv.zip ./data/X_test.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c061b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -Uqq pytorch-lightning torchmetrics wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f724f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from tqdm.auto import tqdm \n",
    "from kaggle import api\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torchmetrics\n",
    "\n",
    "from pylab import rcParams\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f733dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlinBackend.figure_format = 'retina'\n",
    "sns.set_style(style= 'whitegrid')\n",
    "rcParams['figure.figsize'] = 14,9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0429b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e50e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53ce410b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          row_id  series_id  ...  linear_acceleration_Y  linear_acceleration_Z\n",
      "0            0_0          0  ...                 2.1030                -9.7532\n",
      "1            0_1          0  ...                 1.5064                -9.4128\n",
      "2            0_2          0  ...                 1.5922                -8.7267\n",
      "3            0_3          0  ...                 1.0993               -10.0960\n",
      "4            0_4          0  ...                 1.4689               -10.4410\n",
      "...          ...        ...  ...                    ...                    ...\n",
      "487675  3809_123       3809  ...                 2.0115                -9.0063\n",
      "487676  3809_124       3809  ...                 3.0696                -8.1257\n",
      "487677  3809_125       3809  ...                 4.2622                -8.1443\n",
      "487678  3809_126       3809  ...                 4.7130                -9.4435\n",
      "487679  3809_127       3809  ...                 4.2751               -10.4980\n",
      "\n",
      "[487680 rows x 13 columns]"
     ]
    }
   ],
   "source": [
    "x_train = pd.read_csv('./data/raw/X_train.csv', low_memory=False)\n",
    "y_train = pd.read_csv('./data/raw/y_train.csv', low_memory=False)\n",
    "x_test = pd.read_csv('./data/raw/X_test.csv', low_memory=False)\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9590484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    x_train = pd.read_csv('./data/raw/X_train.csv', low_memory=False)\n",
    "    y_train = pd.read_csv('./data/raw/y_train.csv', low_memory=False)\n",
    "    x_test = pd.read_csv('./data/raw/X_test.csv', low_memory=False) \n",
    "\n",
    "    data = [x_train, y_train, x_test]\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64940e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    x_train = pd.read_csv('./data/raw/X_train.csv', low_memory=False)\n",
    "    y_train = pd.read_csv('./data/raw/y_train.csv', low_memory=False)\n",
    "    x_test = pd.read_csv('./data/raw/X_test.csv', low_memory=False) \n",
    "\n",
    "    data = [[x_train, y_train], x_test]\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf323e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    x_train = pd.read_csv('./data/raw/X_train.csv', low_memory=False)\n",
    "    y_train = pd.read_csv('./data/raw/y_train.csv', low_memory=False)\n",
    "    x_test = pd.read_csv('./data/raw/X_test.csv', low_memory=False) \n",
    "\n",
    "    data = [(x_train, y_train),(x_test)]\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "210803f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a358785",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = (1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1586800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = (1, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07e75758",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d25ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, = (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a002c87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d9b6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, = (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb4cee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fc3b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cafa33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_log():\n",
    "    with wandb.init(project='surface_pred', job_type='load-raw-data') as run:\n",
    "        all_data = load()\n",
    "        names = ['training', 'test']\n",
    "\n",
    "        #create our artifact\n",
    "        raw_data = wandb.Artifact('surface-raw', type='dataset',\n",
    "                                 description = 'Raw dataset containin x_train and y_train for the training, and x_test for the testing',\n",
    "                                 metadata = {'source': 'kaggle.com/career-con-2019',\n",
    "                                             'sizes': [len(data[0]) for data in all_data]\n",
    "                                             })\n",
    "\n",
    "        for name, data in zip(names, all_data):\n",
    "            #store a new file to the created artifact\n",
    "            with raw_data.new_file(name + '.pt', mode='wb') as file:\n",
    "                if name == 'training':\n",
    "                    x, y = data\n",
    "                    torch.save((x,y), file)\n",
    "                else:\n",
    "                    x, = data\n",
    "                    torch.save((x), file)\n",
    "\n",
    "        run.log_artifact(raw_data)\n",
    "\n",
    "load_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b2a8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    x_train = pd.read_csv('./data/raw/X_train.csv', low_memory=False)\n",
    "    y_train = pd.read_csv('./data/raw/y_train.csv', low_memory=False)\n",
    "    x_test = pd.read_csv('./data/raw/X_test.csv', low_memory=False) \n",
    "\n",
    "    data = [(x_train, y_train),(x_test, )]\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df94798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_log():\n",
    "    with wandb.init(project='surface_pred', job_type='load-raw-data') as run:\n",
    "        all_data = load()\n",
    "        names = ['training', 'test']\n",
    "\n",
    "        #create our artifact\n",
    "        raw_data = wandb.Artifact('surface-raw', type='dataset',\n",
    "                                 description = 'Raw dataset containin x_train and y_train for the training, and x_test for the testing',\n",
    "                                 metadata = {'source': 'kaggle.com/career-con-2019',\n",
    "                                             'sizes': [len(data[0]) for data in all_data]\n",
    "                                             })\n",
    "\n",
    "        for name, data in zip(names, all_data):\n",
    "            #store a new file to the created artifact\n",
    "            with raw_data.new_file(name + '.pt', mode='wb') as file:\n",
    "                if name == 'training':\n",
    "                    x, y = data\n",
    "                    torch.save((x,y), file)\n",
    "                else:\n",
    "                    x, = data\n",
    "                    torch.save((x), file)\n",
    "\n",
    "        run.log_artifact(raw_data)\n",
    "\n",
    "load_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4201f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('phade160/surface_pred/surface-raw:v0', type='dataset')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "713b882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./artifacts/surface-raw:v0'"
     ]
    }
   ],
   "source": [
    "artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a7c2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9e7ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./artifacts/surface-raw:v0/training.pt'"
     ]
    }
   ],
   "source": [
    "os.path.join(artifact_dir, 'training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f67b57d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(          row_id  series_id  ...  linear_acceleration_Y  linear_acceleration_Z\n",
      " 0            0_0          0  ...                 2.1030                -9.7532\n",
      " 1            0_1          0  ...                 1.5064                -9.4128\n",
      " 2            0_2          0  ...                 1.5922                -8.7267\n",
      " 3            0_3          0  ...                 1.0993               -10.0960\n",
      " 4            0_4          0  ...                 1.4689               -10.4410\n",
      " ...          ...        ...  ...                    ...                    ...\n",
      " 487675  3809_123       3809  ...                 2.0115                -9.0063\n",
      " 487676  3809_124       3809  ...                 3.0696                -8.1257\n",
      " 487677  3809_125       3809  ...                 4.2622                -8.1443\n",
      " 487678  3809_126       3809  ...                 4.7130                -9.4435\n",
      " 487679  3809_127       3809  ...                 4.2751               -10.4980\n",
      " \n",
      " [487680 rows x 13 columns],       series_id  group_id        surface\n",
      " 0             0        13  fine_concrete\n",
      " 1             1        31       concrete\n",
      " 2             2        20       concrete\n",
      " 3             3        31       concrete\n",
      " 4             4        22     soft_tiles\n",
      " ...         ...       ...            ...\n",
      " 3805       3805        55          tiled\n",
      " 3806       3806        67           wood\n",
      " 3807       3807        48  fine_concrete\n",
      " 3808       3808        54          tiled\n",
      " 3809       3809        56       soft_pvc\n",
      " \n",
      " [3810 rows x 3 columns])"
     ]
    }
   ],
   "source": [
    "torch.load(os.path.join(artifact_dir, 'training.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ce686e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(x, y=None): \n",
    "  sequences = []\n",
    "  for series_id, group in x.groupby('series_id'):\n",
    "    sequence_features = group[feature_columns]\n",
    "    if y:\n",
    "        label = y[y.series_id == series_id].iloc[0].label\n",
    "        sequences.append((sequence_features, label))\n",
    "      \n",
    "    else:\n",
    "        sequences.append(sequence_features)\n",
    "\n",
    "  return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9b1c0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          row_id  series_id  ...  linear_acceleration_Y  linear_acceleration_Z\n",
      "0            0_0          0  ...                 2.8027                -9.6816\n",
      "1            0_1          0  ...                 2.5408                -9.8521\n",
      "2            0_2          0  ...                 2.5853                -9.3835\n",
      "3            0_3          0  ...                 2.9966                -8.7415\n",
      "4            0_4          0  ...                 2.6498                -8.8432\n",
      "...          ...        ...  ...                    ...                    ...\n",
      "488443  3815_123       3815  ...                 3.5421                -8.4445\n",
      "488444  3815_124       3815  ...                 3.3380                -8.8012\n",
      "488445  3815_125       3815  ...                 3.2110                -9.3700\n",
      "488446  3815_126       3815  ...                 2.8634                -9.8546\n",
      "488447  3815_127       3815  ...                 2.5159               -10.5870\n",
      "\n",
      "[488448 rows x 13 columns]"
     ]
    }
   ],
   "source": [
    "torch.load(os.path.join(artifact_dir, 'test.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28be8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_log():\n",
    "    with wandb.init(project='surface_pred', job_type='preprocess-data') as run:\n",
    "        preprocessed_data = wandb.Artifact(name='surface-preprocess', type='dataset',\n",
    "                                            description='get sequences from raw data and split train sequences to train val',\n",
    "                                            metadata={'get_sequences': True,\n",
    "                                                      'split_train' : True})\n",
    "        # Declare which artifact to be used\n",
    "        raw_data_artifact = run.use_artifact('surface-raw:latest')\n",
    "        #download it\n",
    "        raw_data = raw_data_artifact.download()\n",
    "        for data in ['training', 'test']:\n",
    "            filename = data + '.pt'\n",
    "            if data == 'taining':\n",
    "                x, y = torch.load(os.path.join(raw_data, filename))\n",
    "                sequences = get_sequences(x,y)\n",
    "                train_sequences, val_sequences = train_test_split(sequences, test_size=0.2)\n",
    "            else:\n",
    "                x = torch.load(os.path.join(raw_data, filename))\n",
    "                test_sequences = get_sequences(x)\n",
    "            with preprocessed_data.new_file(data + '.pt', mode='wb') as file:\n",
    "                if data == 'taining':\n",
    "                    torch.save((train_sequences, val_sequences), file)\n",
    "\n",
    "                else:\n",
    "                    torch.save(test_sequences)\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3482f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_log():\n",
    "    with wandb.init(project='surface_pred', job_type='preprocess-data') as run:\n",
    "        preprocessed_data = wandb.Artifact(name='surface-preprocess', type='dataset',\n",
    "                                            description='get sequences from raw data and split train sequences to train val',\n",
    "                                            metadata={'get_sequences': True,\n",
    "                                                      'split_train' : True})\n",
    "        # Declare which artifact to be used\n",
    "        raw_data_artifact = run.use_artifact('surface-raw:latest')\n",
    "        #download it\n",
    "        raw_data = raw_data_artifact.download()\n",
    "        for data in ['training', 'test']:\n",
    "            filename = data + '.pt'\n",
    "            if data == 'taining':\n",
    "                x, y = torch.load(os.path.join(raw_data, filename))\n",
    "                sequences = get_sequences(x,y)\n",
    "                train_sequences, val_sequences = train_test_split(sequences, test_size=0.2)\n",
    "            else:\n",
    "                x = torch.load(os.path.join(raw_data, filename))\n",
    "                test_sequences = get_sequences(x)\n",
    "            with preprocessed_data.new_file(data + '.pt', mode='wb') as file:\n",
    "                if data == 'taining':\n",
    "                    torch.save((train_sequences, val_sequences), file)\n",
    "\n",
    "                else:\n",
    "                    torch.save(test_sequences)\n",
    "\n",
    "        run.log_artifact(preprocessed_data)\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44f3109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c922e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_log():\n",
    "    with wandb.init(project='surface_pred', job_type='preprocess-data') as run:\n",
    "        preprocessed_data = wandb.Artifact(name='surface-preprocess', type='dataset',\n",
    "                                            description='get sequences from raw data and split train sequences to train val',\n",
    "                                            metadata={'get_sequences': True,\n",
    "                                                      'split_train' : True})\n",
    "        # Declare which artifact to be used\n",
    "        raw_data_artifact = run.use_artifact('surface-raw:latest')\n",
    "        #download it\n",
    "        raw_data = raw_data_artifact.download()\n",
    "        for data in ['training', 'test']:\n",
    "            filename = data + '.pt'\n",
    "            if data == 'taining':\n",
    "                x, y = torch.load(os.path.join(raw_data, filename))\n",
    "                sequences = get_sequences(x,y)\n",
    "                train_sequences, val_sequences = train_test_split(sequences, test_size=0.2)\n",
    "            else:\n",
    "                x, = torch.load(os.path.join(raw_data, filename))\n",
    "                test_sequences = get_sequences(x)\n",
    "            with preprocessed_data.new_file(data + '.pt', mode='wb') as file:\n",
    "                if data == 'taining':\n",
    "                    torch.save((train_sequences, val_sequences), file)\n",
    "\n",
    "                else:\n",
    "                    torch.save(test_sequences)\n",
    "\n",
    "        run.log_artifact(preprocessed_data)\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2827e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18b426c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_log():\n",
    "    with wandb.init(project='surface_pred', job_type='preprocess-data') as run:\n",
    "        preprocessed_data = wandb.Artifact(name='surface-preprocess', type='dataset',\n",
    "                                            description='get sequences from raw data and split train sequences to train val',\n",
    "                                            metadata={'get_sequences': True,\n",
    "                                                      'split_train' : True})\n",
    "        # Declare which artifact to be used\n",
    "        raw_data_artifact = run.use_artifact('surface-raw:latest')\n",
    "        #download it\n",
    "        raw_data = raw_data_artifact.download()\n",
    "        \n",
    "        for data in ['training', 'test']:\n",
    "            filename = data + '.pt'\n",
    "            if data == 'taining':\n",
    "                x, y = torch.load(os.path.join(raw_data, filename))\n",
    "                sequences = get_sequences(x,y)\n",
    "                train_sequences, val_sequences = train_test_split(sequences, test_size=0.2)\n",
    "            else:\n",
    "                x = torch.load(os.path.join(raw_data, filename))\n",
    "                test_sequences = get_sequences(x)\n",
    "            with preprocessed_data.new_file(data + '.pt', mode='wb') as file:\n",
    "                if data == 'taining':\n",
    "                    torch.save((train_sequences, val_sequences), file)\n",
    "\n",
    "                else:\n",
    "                    torch.save(test_sequences)\n",
    "\n",
    "        run.log_artifact(preprocessed_data)\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47f0c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_log():\n",
    "    with wandb.init(project='surface_pred', job_type='preprocess-data') as run:\n",
    "        preprocessed_data = wandb.Artifact(name='surface-preprocess', type='dataset',\n",
    "                                            description='get sequences from raw data and split train sequences to train val',\n",
    "                                            metadata={'get_sequences': True,\n",
    "                                                      'split_train' : True})\n",
    "        # Declare which artifact to be used\n",
    "        raw_data_artifact = run.use_artifact('surface-raw:latest')\n",
    "        #download it\n",
    "        raw_data = raw_data_artifact.download()\n",
    "        \n",
    "        for data in ['training', 'test']:\n",
    "            filename = data + '.pt'\n",
    "            if data == 'taining':\n",
    "                x, y = torch.load(os.path.join(raw_data, filename))\n",
    "                sequences = get_sequences(x,y)\n",
    "                train_sequences, val_sequences = train_test_split(sequences, test_size=0.2)\n",
    "            else:\n",
    "                x = torch.load(os.path.join(raw_data, filename))\n",
    "                test_sequences = get_sequences(x[0])\n",
    "            with preprocessed_data.new_file(data + '.pt', mode='wb') as file:\n",
    "                if data == 'taining':\n",
    "                    torch.save((train_sequences, val_sequences), file)\n",
    "\n",
    "                else:\n",
    "                    torch.save(test_sequences)\n",
    "\n",
    "        run.log_artifact(preprocessed_data)\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c523112",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ed9c179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orientation_X',\n",
      " 'orientation_Y',\n",
      " 'orientation_Z',\n",
      " 'orientation_W',\n",
      " 'angular_velocity_X',\n",
      " 'angular_velocity_Y',\n",
      " 'angular_velocity_Z',\n",
      " 'linear_acceleration_X',\n",
      " 'linear_acceleration_Y',\n",
      " 'linear_acceleration_Z']"
     ]
    }
   ],
   "source": [
    "#we dont need the first 3 cols\n",
    "feature_columns = x_train.columns.to_list()[3:]\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca3bd173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_log():\n",
    "    with wandb.init(project='surface_pred', job_type='preprocess-data') as run:\n",
    "        preprocessed_data = wandb.Artifact(name='surface-preprocess', type='dataset',\n",
    "                                            description='get sequences from raw data and split train sequences to train val',\n",
    "                                            metadata={'get_sequences': True,\n",
    "                                                      'split_train' : True})\n",
    "        # Declare which artifact to be used\n",
    "        raw_data_artifact = run.use_artifact('surface-raw:latest')\n",
    "        #download it\n",
    "        raw_data = raw_data_artifact.download()\n",
    "        \n",
    "        for data in ['training', 'test']:\n",
    "            filename = data + '.pt'\n",
    "            if data == 'taining':\n",
    "                x, y = torch.load(os.path.join(raw_data, filename))\n",
    "                sequences = get_sequences(x,y)\n",
    "                train_sequences, val_sequences = train_test_split(sequences, test_size=0.2)\n",
    "            else:\n",
    "                x = torch.load(os.path.join(raw_data, filename))\n",
    "                test_sequences = get_sequences(x[0])\n",
    "            with preprocessed_data.new_file(data + '.pt', mode='wb') as file:\n",
    "                if data == 'taining':\n",
    "                    torch.save((train_sequences, val_sequences), file)\n",
    "\n",
    "                else:\n",
    "                    torch.save(test_sequences)\n",
    "\n",
    "        run.log_artifact(preprocessed_data)\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36ec2211",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03117a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_log():\n",
    "    with wandb.init(project='surface_pred', job_type='preprocess-data') as run:\n",
    "        preprocessed_data = wandb.Artifact(name='surface-preprocess', type='dataset',\n",
    "                                            description='get sequences from raw data and split train sequences to train val',\n",
    "                                            metadata={'get_sequences': True,\n",
    "                                                      'split_train' : True})\n",
    "        # Declare which artifact to be used\n",
    "        raw_data_artifact = run.use_artifact('surface-raw:latest')\n",
    "        #download it\n",
    "        raw_data = raw_data_artifact.download()\n",
    "        \n",
    "        for data in ['training', 'test']:\n",
    "            filename = data + '.pt'\n",
    "            if data == 'taining':\n",
    "                x, y = torch.load(os.path.join(raw_data, filename))\n",
    "                sequences = get_sequences(x,y)\n",
    "                train_sequences, val_sequences = train_test_split(sequences, test_size=0.2)\n",
    "            else:\n",
    "                x = torch.load(os.path.join(raw_data, filename))\n",
    "                test_sequences = get_sequences(x[0])\n",
    "            with preprocessed_data.new_file(data + '.pt', mode='wb') as file:\n",
    "                if data == 'taining':\n",
    "                    torch.save((train_sequences, val_sequences), file)\n",
    "\n",
    "                else:\n",
    "                    torch.save(test_sequences, file)\n",
    "\n",
    "        run.log_artifact(preprocessed_data)\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0fcbfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "972e2de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_log():\n",
    "    with wandb.init(project='surface_pred', job_type='preprocess-data') as run:\n",
    "        preprocessed_data = wandb.Artifact(name='surface-preprocess', type='dataset',\n",
    "                                            description='get sequences from raw data and split train sequences to train val',\n",
    "                                            metadata={'get_sequences': True,\n",
    "                                                      'split_train' : True})\n",
    "        # Declare which artifact to be used\n",
    "        raw_data_artifact = run.use_artifact('surface-raw:latest')\n",
    "        #download it\n",
    "        raw_data = raw_data_artifact.download()\n",
    "        \n",
    "        for data in ['training', 'test']:\n",
    "            filename = data + '.pt'\n",
    "            if data == 'taining':\n",
    "                x, y = torch.load(os.path.join(raw_data, filename))\n",
    "                sequences = get_sequences(x,y)\n",
    "                train_sequences, val_sequences = train_test_split(sequences, test_size=0.2)\n",
    "            else:\n",
    "                x = torch.load(os.path.join(raw_data, filename))\n",
    "                test_sequences = get_sequences(x)\n",
    "            with preprocessed_data.new_file(data + '.pt', mode='wb') as file:\n",
    "                if data == 'taining':\n",
    "                    torch.save((train_sequences, val_sequences), file)\n",
    "\n",
    "                else:\n",
    "                    torch.save(test_sequences, file)\n",
    "\n",
    "        run.log_artifact(preprocessed_data)\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "828a2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf6e3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_log():\n",
    "    with wandb.init(project='surface_pred', job_type='preprocess-data') as run:\n",
    "        preprocessed_data = wandb.Artifact(name='surface-preprocess', type='dataset',\n",
    "                                            description='get sequences from raw data and split train sequences to train val',\n",
    "                                            metadata={'get_sequences': True,\n",
    "                                                      'split_train' : True})\n",
    "        # Declare which artifact to be used\n",
    "        raw_data_artifact = run.use_artifact('surface-raw:latest')\n",
    "        #download it\n",
    "        raw_data = raw_data_artifact.download()\n",
    "        \n",
    "        for data in ['training', 'test']:\n",
    "            filename = data + '.pt'\n",
    "            if data == 'taining':\n",
    "                x, y = torch.load(os.path.join(raw_data, filename))\n",
    "                sequences = get_sequences(x,y)\n",
    "                train_sequences, val_sequences = train_test_split(sequences, test_size=0.2)\n",
    "            else:\n",
    "                x = torch.load(os.path.join(raw_data, filename))\n",
    "                print(x)\n",
    "                test_sequences = get_sequences(x)\n",
    "            with preprocessed_data.new_file(data + '.pt', mode='wb') as file:\n",
    "                if data == 'taining':\n",
    "                    torch.save((train_sequences, val_sequences), file)\n",
    "\n",
    "                else:\n",
    "                    torch.save(test_sequences, file)\n",
    "\n",
    "        run.log_artifact(preprocessed_data)\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cf80b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/phade160/surface_pred/runs/39snf97u\" target=\"_blank\">denim-waterfall-49</a></strong> to <a href=\"https://wandb.ai/phade160/surface_pred\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocess_and_log()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
